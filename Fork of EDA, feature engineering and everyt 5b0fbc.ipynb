{"cells":[{"metadata":{"_uuid":"6daa1434e9a463cb819f93bb08b41602e4b1f64b"},"cell_type":"markdown","source":"Reference\n\n\nhttps://www.kaggle.com/artgor/eda-feature-engineering-and-everything\n\nhttps://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n\nhttps://www.kaggle.com/christofhenkel/market-data-nn-baseline"},{"metadata":{"_uuid":"896f2b18bf4f32ec7dfb0196e3d718a7ae991b6a"},"cell_type":"markdown","source":"   ### Getting data and importing libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import gc\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"900eef774de681826e9aa007c6f284c90c94d4b0"},"cell_type":"code","source":"#from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import GradientBoostingClassifier\n#from sklearn.kernel_ridge import KernelRidge\n#from sklearn.svm import NuSVR\n#from sklearn.pipeline import make_pipeline\n#from sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\n#import xgboost as xgb\nimport lightgbm as lgb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# official way to get the data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\nprint('Done!')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1ee6824cf41c4fd03be113d54e6975cf3574c06f"},"cell_type":"code","source":"(market_train_df, news_train_df) = env.get_training_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eabe7d13057c6fb5c0eeac667c34b8a42d985ae1"},"cell_type":"code","source":"pd.set_option(\"display.max_rows\",10)\nmarket_train_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a21a4bd4a0be55614fcdcbc8f1812b5994dabb19"},"cell_type":"markdown","source":"## Market data\n\nWe have a really interesting dataset which contains stock prices for many companies over a decade!\n\nFor now let's have a look at the data itself and not think about the competition. We can see long-term trends, appearing and declining companies and many other things."},{"metadata":{"trusted":true,"_uuid":"cca04f17e12924251a4ea85f1ea63a81e4c1c4eb"},"cell_type":"code","source":"print(f'{market_train_df.shape[0]} samples and {market_train_df.shape[1]} features in the training market dataset.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"8ca5182693d1826b459101b54dd23f6d7bf69b3d"},"cell_type":"code","source":"\nmarket_train_df['price_diff'] = market_train_df['close'] - market_train_df['open']\ngrouped = market_train_df.groupby('time').agg({'price_diff': ['std', 'min']}).reset_index()\nmarket_train_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a21c4b16636acb7cc98caa354bb06ea989d6373f","_kg_hide-input":true,"scrolled":false},"cell_type":"code","source":"print(f\"Average standard deviation of price change within a day in {grouped['price_diff']['std'].mean():.4f}.\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c7e55d1b2db816c00d5cc66f373949ec010c0494"},"cell_type":"markdown","source":"We can see huge price fluctiations when market crashed. Just think about it... **But this is wrong!** There was no huge crash on January 2010... Let's dive into the data!"},{"metadata":{"_uuid":"f4c43e1d3dc085b540af9736043089f5fb386f6b"},"cell_type":"markdown","source":"### Possible data errors\n\nAt first let's simply sort data by the difference between open and close prices."},{"metadata":{"trusted":true,"_uuid":"fd332dac81171201abbefcf3a42cc0a2c315d895"},"cell_type":"code","source":"market_train_df.sort_values('price_diff')[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"04fe6a44a65a7b66fa128f24acf6717eda1f6e20"},"cell_type":"code","source":"market_train_df['close_to_open'] =  np.abs(market_train_df['close'] / market_train_df['open'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"39bf65e0c1a6fac123f5af29a76248c21c458747"},"cell_type":"code","source":"print(f\"In {(market_train_df['close_to_open'] >= 1.2).sum()} lines price increased by 20% or more.\")\nprint(f\"In {(market_train_df['close_to_open'] <= 0.8).sum()} lines price decreased by 20% or more.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"549aabe483a6f62dd7946020d3d8e60be89919d3"},"cell_type":"markdown","source":"Well, this isn't much considering we have more than 4 million lines and a lot of these cases are due to price falls during market crash. Well just need to deal with outliers."},{"metadata":{"trusted":true,"_uuid":"47fcd3e5635e68ea9681626bfb3bc9583b76fb00"},"cell_type":"code","source":"print(f\"In {(market_train_df['close_to_open'] >= 2).sum()} lines price increased by 100% or more.\")\nprint(f\"In {(market_train_df['close_to_open'] <= 0.5).sum()} lines price decreased by 100% or more.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78b680921bead3d610d3ba35f6cefa98778edeff"},"cell_type":"markdown","source":"For a quick fix I'll replace outliers in these lines with mean open or close price of this company."},{"metadata":{"_kg_hide-input":true,"trusted":true,"scrolled":true,"_uuid":"19ef8496d92912fd56dce27ea0548c8a42c92212"},"cell_type":"code","source":"\nmarket_train_df['assetName_mean_open'] = market_train_df.groupby('assetName')['open'].transform('mean')\nmarket_train_df['assetName_mean_close'] = market_train_df.groupby('assetName')['close'].transform('mean')\n\n# if open price is too far from mean open price for this company, replace it. Otherwise replace close price.\nfor i, row in market_train_df.loc[market_train_df['close_to_open'] >= 2].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']\n        \nfor i, row in market_train_df.loc[market_train_df['close_to_open'] <= 0.5].iterrows():\n    if np.abs(row['assetName_mean_open'] - row['open']) > np.abs(row['assetName_mean_close'] - row['close']):\n        market_train_df.iloc[i,5] = row['assetName_mean_open']\n    else:\n        market_train_df.iloc[i,4] = row['assetName_mean_close']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"db04b466e52606a2ff406ad30522cab921e8ea89"},"cell_type":"code","source":"market_train_df.drop(columns=['price_diff', 'close_to_open', 'assetName_mean_open', 'assetName_mean_close'], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"84759fb0c61f4e0e799ebfadfac2c923856b127c"},"cell_type":"markdown","source":"## News data\n\nreference\nhttps://www.kaggle.com/jsaguiar/baseline-with-news"},{"metadata":{"_uuid":"272aa537ee53b60a7fe6c5e10f410427035e76c6"},"cell_type":"markdown","source":"## 2. Preprocessing News\nWe are going to remove some columns for now and apply label encoding to a few others:"},{"metadata":{"trusted":true,"_uuid":"b79fc6443d5c3e914334506e038e45bcf3d925ec"},"cell_type":"code","source":"#news_train_df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c2791b79b1872afc642009d1c9a728500fbec082"},"cell_type":"code","source":"#news_num_cols = ['marketCommentary',\n#                   'sentenceCount', 'wordCount',\n#                   'firstMentionSentence', 'relevance', 'sentimentClass',\n#                   'sentimentNegative', 'sentimentNeutral', 'sentimentPositive',\n#                   'sentimentWordCount']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aed9f532f6a648ba657c1e2db7c551a6e792ce35"},"cell_type":"code","source":"#news_train_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1049d43c53f275ccc11bf6d2be56ff28242438ef"},"cell_type":"code","source":"'''\ndef preprocess_news(news_train):\n    drop_list = [\n        'audiences', 'subjects', 'assetName',\n        'headline', 'firstCreated', 'sourceTimestamp','headlineTag', 'provider', 'sourceId',\n        'noveltyCount12H', 'noveltyCount24H',\n        'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H',\n        'volumeCounts24H', 'volumeCounts3D', 'volumeCounts5D',\n        'volumeCounts7D'\n    ]\n    news_train.drop(drop_list, axis=1, inplace=True)\n    \n    news_train[news_num_cols] = news_train[news_num_cols].fillna(news_train[news_num_cols].mean())\n    # Factorize categorical columns\n    #for col in ['headlineTag', 'provider', 'sourceId']:\n    #    news_train[col], uniques = pd.factorize(news_train[col])\n    #    del uniques\n    \n    # Remove {} and '' from assetCodes column\n    news_train['assetCodes'] = news_train['assetCodes'].apply(lambda x: x[1:-1].replace(\"'\", \"\"))\n    return news_train\n\nnews_train_df = preprocess_news(news_train_df)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0101772a761addd2a5d0bee5064da4ca6eb1ccc1"},"cell_type":"code","source":"#news_train_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f7ea16112e99135d3196646384078795c10ccb07"},"cell_type":"code","source":"'''\ndef unstack_asset_codes(news_train):\n    codes = []\n    indexes = []\n    for i, values in news_train['assetCodes'].iteritems():\n        explode = values.split(\", \")\n        codes.extend(explode)\n        repeat_index = [int(i)]*len(explode)\n        indexes.extend(repeat_index)\n    index_df = pd.DataFrame({'news_index': indexes, 'assetCode': codes})\n    del codes, indexes\n    gc.collect()\n    return index_df\n\nindex_df = unstack_asset_codes(news_train_df)\nindex_df.head()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"367836be6169f8308183b15583070f7987bb90b3"},"cell_type":"code","source":"'''\ndef merge_news_on_index(news_train, index_df):\n    news_train['news_index'] = news_train.index.copy()\n\n    # Merge news on unstacked assets\n    news_unstack = index_df.merge(news_train, how='left', on='news_index')\n    news_unstack.drop(['news_index', 'assetCodes'], axis=1, inplace=True)\n    return news_unstack\n\nnews_unstack = merge_news_on_index(news_train_df, index_df)\ndel news_train_df, index_df\ngc.collect()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46c652ae0c4a822f212d8af303b12149dfe8cb0f"},"cell_type":"code","source":"'''\ndef group_news(news_frame):\n    news_frame['date'] = news_frame.time.dt.date  # Add date column\n    \n    aggregations = ['mean']\n    gp = news_frame.groupby(['assetCode', 'date']).agg(aggregations)\n    gp.columns = pd.Index([\"{}_{}\".format(e[0], e[1]) for e in gp.columns.tolist()])\n    gp.reset_index(inplace=True)\n    # Set datatype to float32\n    float_cols = {c: 'float32' for c in gp.columns if c not in ['assetCode', 'date']}\n    return gp.astype(float_cols)\n\nnews_agg = group_news(news_unstack)\ndel news_unstack; gc.collect()\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7f538c0c7542a4f4846065863d4cebd6cfba1fb"},"cell_type":"code","source":"#market_train_df['date'] = market_train_df.time.dt.date\n#market_train_df = market_train_df.merge(news_agg, how='left', on=['assetCode', 'date'])\n#gc.collect()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"94331718f395dd05148a00e3ee7a62b226cfcdd8"},"cell_type":"code","source":"#from datetime import datetime, timedelta\n#start_date = datetime(2009, 1, 1, 0, 0, 0).date()\n#market_train_df = market_train_df[market_train_df.time.dt.date >= start_date].reset_index(drop=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c9297e8c67cdb16bc4213643cca24d2af41967d0"},"cell_type":"code","source":"#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"739f525abb43f1f8f42a8b48ff044123c22297e5"},"cell_type":"code","source":"#market_train_df.sentimentNegative_mean.isnull()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8e784d89a6731a06bc75b5c0ded3730ec6d43701"},"cell_type":"markdown","source":"## Modelling\n\nIt's time to build a model!\nI think that in this case we should build a binary classifier - we will simply predict whether the target goes up or down.\n1. NN\n2. LGBM\n3. CATBOOST\n4. ENSEMBLE(STACKING)"},{"metadata":{"trusted":true,"_uuid":"9695eface6eee79f079e168d97ced1835e868436"},"cell_type":"code","source":"#numerical columns\ncat_cols = ['assetCode']\nnum_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n            'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', \n            'returnsClosePrevMktres10','returnsOpenPrevMktres10']\n\n#new_num_cols = num_cols.append(news_num_cols)\n#num_cols = new_num_cols\nfrom sklearn.model_selection import train_test_split\n\ntrain_indices, val_indices = train_test_split(market_train_df.index.values,test_size=0.25, random_state=23)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"009c25f3a89386bd65a2310b30a5f24df963e788"},"cell_type":"code","source":"def encode(encoder, x):\n    len_encoder = len(encoder)\n    try:\n        id = encoder[x]\n    except KeyError:\n        id = len_encoder\n    return id\n\nencoders = [{} for cat in cat_cols]\n\n\nfor i, cat in enumerate(cat_cols):\n    print('encoding %s ...' % cat, end=' ')\n    encoders[i] = {l: id for id, l in enumerate(market_train_df.loc[train_indices, cat].astype(str).unique())}\n    market_train_df[cat] = market_train_df[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    print('Done')\n\nembed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6b119e0cb450122240da44edbaa3ff6bcd0ac10f"},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n \nmarket_train_df[num_cols] = market_train_df[num_cols].fillna(0)\nprint('scaling numerical columns')\n\nscaler = StandardScaler()\n\n#col_mean = market_train[col].mean()\n#market_train[col].fillna(col_mean, inplace=True)\nscaler = StandardScaler()\nmarket_train_df[num_cols] = scaler.fit_transform(market_train_df[num_cols])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0247345324714119658ba24e6744b9b2a7a0d96d"},"cell_type":"code","source":"market_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3381a6d63097b567aede8e41f85a6396dcf9f965"},"cell_type":"code","source":"def get_input(market_train, indices):\n    X_num = market_train.loc[indices, num_cols].values\n    X = {'num':X_num}\n    for cat in cat_cols:\n        X[cat] = market_train.loc[indices, cat_cols].values\n    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'time'].dt.date\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\nX_train,y_train,r_train,u_train,d_train = get_input(market_train_df, train_indices)\nX_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train_df, val_indices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e0bfcadeabf52118b75f236778d3e04341cdfaed"},"cell_type":"code","source":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization, Dropout\nfrom keras.losses import binary_crossentropy, mse\n\nclass NN_base:        \n        \n    def __init__(self, include_cat=True, n_features=11):\n        \n        if include_cat:\n            categorical_inputs = []\n            for cat in cat_cols:\n                categorical_inputs.append(Input(shape=[1], name=cat))\n\n            categorical_embeddings = []\n            for i, cat in enumerate(cat_cols):\n                categorical_embeddings.append(Embedding(embed_sizes[i], 10)(categorical_inputs[i]))\n\n\n            #categorical_logits = Concatenate()([Flatten()(cat_emb) for cat_emb in categorical_embeddings])\n            categorical_logits = Flatten()(categorical_embeddings[0])\n            categorical_logits = Dense(32,activation='relu')(categorical_logits)\n\n            #categorical_logits = Flatten()(categorical_embeddings[0])\n            #categorical_logits = Dense(32,activation='relu')(categorical_logits)\n            categorical_logits = Dropout(0.5)(categorical_logits)\n            categorical_logits = BatchNormalization()(categorical_logits)\n            categorical_logits = Dense(32,activation='relu')(categorical_logits)\n        \n        \n        numerical_inputs = Input(shape=(n_features,), name='num')\n        numerical_logits = numerical_inputs\n        numerical_logits = BatchNormalization()(numerical_logits)\n\n        numerical_logits = Dense(128,activation='relu')(numerical_logits)\n        numerical_logits = Dropout(0.5)(numerical_logits)\n        numerical_logits = BatchNormalization()(numerical_logits)\n        #numerical_logits = Dense(128,activation='relu')(numerical_logits)\n        #numerical_logits = Dense(64,activation='relu')(numerical_logits)\n         \n        numerical_logits = Dense(128,activation='relu')(numerical_logits)\n        numerical_logits = Dense(64,activation='relu')(numerical_logits)\n\n        if include_cat:\n            logits = Concatenate()([numerical_logits,categorical_logits])\n        else:\n            logits = numerical_logits\n        logits = Dense(64,activation='relu')(logits)\n        out = Dense(1, activation='sigmoid')(logits)\n        \n        if include_cat:\n            self.model = Model(inputs = categorical_inputs + [numerical_inputs], outputs=out)\n        else:\n            self.model = Model(inputs = [numerical_inputs], outputs=out)\n        self.model.compile(optimizer='adam',loss=binary_crossentropy)\n        \n    def fit(self,X_train,y_train,epochs=3, validation_data_=None):\n        from keras.callbacks import EarlyStopping, ModelCheckpoint\n\n        check_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\n        early_stop = EarlyStopping(patience=5,verbose=True)\n        return self.model.fit(X_train,y_train.astype(int),\n                  validation_data=validation_data_,\n                  epochs=epochs,\n                  batch_size=10000,\n                  verbose=True,\n                  callbacks=[early_stop,check_point]) \n    \n    def predict(self,X_test):\n        return self.model.predict(X_test)\n    \n    def summary(self):\n        self.model.summary()\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"63e1e7c7b82db16f6e0250098b64deb758d81da4"},"cell_type":"code","source":"#model_lgb_tmp = lgb.LGBMClassifier(objective='binary',learning_rate=0.05, bagging_fraction = 0.8,\n#                                bagging_freq = 5, n_estimators=10,boosting_type = 'dart',\n#                                num_leaves = 2452, min_child_samples = 212, reg_lambda=0.01)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5bad2112da5d5d541e874bae66f5f28d6d48b9a1"},"cell_type":"code","source":"#model_lgb_tmp.fit(X_train['num'],y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b71d6b5dc2e8fe91d1ef63767cf331ca4f60084"},"cell_type":"code","source":"#model_xgb_tmp = xgb.XGBClassifier(colsample_bytree=0.4603, gamma=0.0468, \n#                             learning_rate=0.05, max_depth=6, \n#                             min_child_weight=1.7817, n_estimators=10,\n#                             reg_alpha=0.4640, reg_lambda=0.8571,\n#                             subsample=0.5213, silent=1,\n#                             random_state =7, nthread = -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2ecc1870bc8a4ca0841060cfec1f57bc289af8f8"},"cell_type":"code","source":"#model_xgb_tmp.fit(X_train['num'],y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e45fb560dcafbecf2471ea3a076cbba1e34b042c"},"cell_type":"code","source":"'''\nfrom sklearn.metrics import accuracy_score\nconfidence_valid_lgb_only = model_lgb_tmp.predict(X_valid['num'])[:]*2 -1\nprint(accuracy_score(confidence_valid_lgb_only>0,y_valid))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c288bef860cfceec1589786349760bcfa063803"},"cell_type":"code","source":"'''\nfrom sklearn.metrics import accuracy_score\nconfidence_valid_numNN = NN_num.predict(X_valid['num'])[:,0]*2 -1\nprint(accuracy_score(confidence_valid_lgb_only>0,y_valid))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ef106ad5175a3f450a64d4ebaae74ac399273c0b"},"cell_type":"code","source":"'''\n# calculation of actual metric that is used to calculate final score\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid_numNN * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean / std\nprint(score_valid)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc59ce5ca2a4bb485ab7f8c8b0e1d314b4710e4b","scrolled":true},"cell_type":"code","source":"#import gc\n#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7f0189f26494311f08497ecd734d8820f0a7a639"},"cell_type":"code","source":"#from catboost import CatBoostClassifier\n#t_cat = CatBoostClassifier(thread_count=4, n_estimators=500, max_depth=8, eta=0.1, loss_function='Logloss' , verbose=10)\n#t_cat.fit(X_train['num'],y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1d2cc39e2162638b359340750dc28ada5f9372e4"},"cell_type":"code","source":"#confidence_valid=t_cat.predict(X_valid['num'])[:]*2-1\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1796d7cd008910946e76e08aaaeaaa59fc832fdc"},"cell_type":"code","source":"#confidence_valid","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"129fe68bf760a38c0403e07db30f72ff74f40e64"},"cell_type":"code","source":"'''\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(confidence_valid>0,y_valid))\n\n# calculation of actual metric that is used to calculate final score\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean / std\nprint(score_valid)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6fde35dc2fe079ba6e4bd9c0e3d124764f6fb5cd","scrolled":false},"cell_type":"code","source":"'''\nfrom catboost import CatBoostClassifier\n# these are tuned params I found\nx_1 = [0.19000424246380565, 2452, 212, 328, 202]#328\nx_2 = [0.19016805202090095, 2583, 213, 312, 220]#312\n\nparams_1 = {\n        'task': 'train',\n        'boosting_type': 'dart',\n        'objective': 'binary',\n        'learning_rate': x_1[0],\n        'num_leaves': x_1[1],\n        'min_data_in_leaf': x_1[2],\n        'num_iteration': x_1[3],\n        'max_bin': x_1[4],\n        'verbose': 1\n    }\n\nparams_2 = {\n        'task': 'train',\n        'boosting_type': 'dart',\n        'objective': 'binary',\n        'learning_rate': x_2[0],\n        'num_leaves': x_2[1],\n        'min_data_in_leaf': x_2[2],\n        'num_iteration': x_2[3],\n        'max_bin': x_2[4],\n        'verbose': 1\n    }\n\nN_fold = 6\nX_num = X_train['num']\nX_cat = X_train['assetCode']\nkfold = KFold(n_splits=N_fold, shuffle=True, random_state=156)\ntest_data = lgb.Dataset(X_valid['num'], label=y_valid.astype(int), free_raw_data=False)\n#lgb_datasets = []\n#for train_index, holdout_index in kfold.split(X_num, y_train):\n#    lgb_datasets.append(lgb.Dataset(X_num[train_index], label=y_train[train_index].astype(int),free_raw_data=False ))\n\nNN_list=[]\nout_of_fold_predictions_NN_1 = np.zeros((X_num.shape[0],1))\nfor train_index, holdout_index in kfold.split(X_num, y_train):\n    instance = NN_base()\n    X_t = {'num': X_num[train_index], 'assetCode': X_cat[train_index]}\n    X_h = {'num': X_num[holdout_index], 'assetCode': X_cat[holdout_index]}\n    instance.fit(X_t, y_train[train_index],epochs=10)\n    NN_list.append(instance)\n    y_pred=instance.predict(X_num[holdout_index])[:,0]\n    out_of_fold_predictions_NN_1[holdout_index, 0] = y_pred\n\n\nctb_list=[]\nout_of_fold_predictions_ctb_1 = np.zeros((X_num.shape[0],1))\nfor train_index, holdout_index in kfold.split(X_num, y_train):\n    instance = CatBoostClassifier(thread_count=4, n_estimators=400, max_depth=8, eta=0.1, loss_function='Logloss' , verbose=10)\n    instance.fit(X_num[train_index], y_train[train_index])\n    ctb_list.append(instance)\n    y_pred=instance.predict_proba(X_num[holdout_index])[:,1]\n    out_of_fold_predictions_ctb_1[holdout_index, 0] = y_pred    \n    \nlgb_list_1=[]\nout_of_fold_predictions_lgb_1 = np.zeros((X_num.shape[0], 1))\n#for i in range(N_fold):\nfor train_index, holdout_index in kfold.split(X_num, y_train):\n    train_data = lgb.Dataset(X_num[train_index], label=y_train[train_index].astype(int),free_raw_data=False )\n    lgb_list_1.append(lgb.train(params_1,train_data,num_boost_round=100, valid_sets=test_data,early_stopping_rounds=5))\n    y_pred = lgb_list_1[i].predict(X_num[holdout_index])\n    out_of_fold_predictions_lgb_1[holdout_index, 0] = y_pred\n                                   \nlgb_list_2=[]\nout_of_fold_predictions_lgb_2 = np.zeros((X_num.shape[0], 1))\n#for i in range(N_fold):\nfor train_index, holdout_index in kfold.split(X_num, y_train):\n    train_data = lgb.Dataset(X_num[train_index], label=y_train[train_index].astype(int),free_raw_data=False )    \n    lgb_list_2.append(lgb.train(params_1,train_data,num_boost_round=100, valid_sets=test_data,early_stopping_rounds=5))\n    y_pred = lgb_list_2[i].predict(X_num[holdout_index])\n    out_of_fold_predictions_lgb_2[holdout_index, 0] = y_pred\n\nbase_learners_NN = [NN_list]\nbase_learners_ctb = [ctb_list]\nbase_learners_ = [lgb_list_1, lgb_list_2]\nraw_features = X_num                                    \nmeta_features = np.concatenate((out_of_fold_predictions_NN_1,out_of_fold_predictions_ctb_1, out_of_fold_predictions_lgb_1, out_of_fold_predictions_lgb_2), axis=1)\nmeta_features = np.concatenate((raw_features , meta_features), axis=1)\n\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4676ac95d6089c0c2484e3bf2c92ae99a3473b25"},"cell_type":"code","source":"'''\nmeta_features_num_valid = np.column_stack([\n            np.column_stack([model.predict(X_valid['num']) for model in base_learners]).mean(axis=1)\n            for base_learners in base_learners_ ])\n\nmeta_features_num_valid_ctb = np.column_stack([\n            np.column_stack([model.predict_proba(X_valid['num']) for model in base_learners]).mean(axis=1)\n            for base_learners in base_learners_ctb ])\n\nmeta_features_num_valid = np.concatenate((meta_features_num_valid_ctb, meta_features_num_valid), axis=1)\n\nraw_features_valid = X_valid['num']\nmeta_features_valid = np.concatenate((raw_features_valid, meta_features_num_valid), axis=1)\nmeta_features_valid = {'num': meta_features_valid, 'assetCode': X_valid['assetCode']}\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8faab5b309af25a0defb55bf645488943e1d013"},"cell_type":"code","source":"'''\n\n\nmeta_features = {'num': meta_features, 'assetCode': X_cat}  \nNN_meta = NN_base(n_features=15)\nhistory=NN_meta.fit(meta_features,y_train, validation_data_=(meta_features_valid,y_valid.astype(int)) )\n\n\ncat_meta = CatBoostClassifier(thread_count=4, n_estimators=200, max_depth=8, eta=0.1, loss_function='Logloss' , verbose=10)\ncat_meta.fit(meta_features['num'],y_train)\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"304fd1035792494ec5813ec753f21ea47dddc128"},"cell_type":"code","source":"'''\nimport matplotlib.pyplot as plt\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.plot(loss)\nplt.plot(val_loss)\n\nplt.legend(['loss','val_loss'])\nplt.show()\n\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b652bd0a49135b50211b416da440d599fe03a96a"},"cell_type":"code","source":"'''\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=3,learning_rate=100)\ntransformed = tsne.fit_transform(raw_features)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"012a7e3b67c04664a70b680431d5e8a50ef4dfa9"},"cell_type":"code","source":"'''\nmeta_features_num_valid = np.column_stack([\n            np.column_stack([model.predict(X_valid['num']) for model in base_learners]).mean(axis=1)\n            for base_learners in base_learners_ ])\n\nmeta_features_num_valid_ctb = np.column_stack([\n            np.column_stack([model.predict_proba(X_valid['num']) for model in base_learners]).mean(axis=1)\n            for base_learners in base_learners_ctb ])\n\nmeta_features_num_valid = np.concatenate((meta_features_num_valid_ctb, meta_features_num_valid), axis=1)\n\nraw_features_valid = X_valid['num']\nmeta_features_valid = np.concatenate((raw_features_valid, meta_features_num_valid), axis=1)\nmeta_features_valid = {'num': meta_features_valid, 'assetCode': X_valid['assetCode']}\n\nconfidence_valid = NN_meta.predict(meta_features_valid)[:,0]*2 -1\n\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d01fa7fc1af7552db48f3ac41ebd4cb81538ca52"},"cell_type":"code","source":"'''\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(confidence_valid>0,y_valid))\n\n# calculation of actual metric that is used to calculate final score\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean / std\nprint(score_valid)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"69c8ab9432b91ffde5933ead718e64477816ab54"},"cell_type":"code","source":"'''\nimport time\nimport copy\n\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5, along=False):\n        \n        self.along = along\n        \n        tmp_num = []\n        tmp_cat = []\n        for ty, model in base_models:\n            if ty == 'num':\n                tmp_num.append(model)\n            elif ty == 'cat':\n                tmp_cat.append(model)\n            else:\n                continue\n                \n        if tmp_num:\n            self.base_models_num = tuple(tmp_num)\n            self.is_num_models = True\n        else:\n            self.base_models_num = tuple([])\n            self.is_num_models = False\n            \n            \n        if tmp_cat:\n            self.base_models_cat = tuple(tmp_cat)\n            self.is_cat_models = True\n        else:\n            self.base_models_cat = tuple([])\n            self.is_cat_models = False   \n            \n        self.meta_model_type = meta_model[0]\n        self.meta_model = meta_model[1]\n        self.n_folds = n_folds\n        \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_num_ = [list() for x in self.base_models_num]\n        self.base_models_cat_ = [list() for x in self.base_models_cat]\n        if self.meta_model_type == 'object':\n            self.meta_model_ = copy.deepcopy(self.meta_model)\n        else:\n            self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        X_num = X['num']\n        X_cat = X['assetCode']\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X['num'].shape[0], len(self.base_models_num)))\n        for i, model in enumerate(self.base_models_num):\n            for train_index, holdout_index in kfold.split(X_num, y):\n                ts = time.time()\n                instance = clone(model)\n                self.base_models_num_[i].append(instance)\n                instance.fit(X_num[train_index], y[train_index])\n                y_pred = instance.predict(X_num[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                print(\"{} model... complete at {}\".format(i,(time.time()-ts)))\n        \n        out_of_fold_predictions_c = np.zeros((X['num'].shape[0], len(self.base_models_cat)))\n        for i, model in enumerate(self.base_models_cat):\n            for train_index, holdout_index in kfold.split(X_cat, y):\n                ts = time.time()\n                instance = copy.deepcopy(model)\n                self.base_models_cat_[i].append(instance)\n                \n                X_t = {'assetCode' : X_cat[train_index], 'num': X_num[train_index]}\n                X_h = {'assetCode' : X_cat[holdout_index], 'num': X_num[holdout_index]}\n                instance.fit(X_t, y[train_index])\n                y_pred = (instance.predict(X_h) > 0.5 )\n                out_of_fold_predictions_c[holdout_index, i] = y_pred.flatten()\n                print(\"{} model... complete at {}\".format(i,(time.time()-ts)))        \n        \n        if self.is_cat_models == True:\n            out_of_fold_predictions = np.concatenate((out_of_fold_predictions, out_of_fold_predictions_c), axis=1)\n        \n        if self.along == True:\n            meta_features = np.concatenate((X_num,out_of_fold_predictions), axis=1)\n        else:\n            meta_features = out_of_fold_predictions\n            \n        \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(meta_features, y)\n        return self\n    \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        X_num = X['num']\n        X_cat = X['assetCode']\n        \n        meta_features_num = np.column_stack([\n            np.column_stack([model.predict(X_num) for model in base_models_num]).mean(axis=1)\n            for base_models_num in self.base_models_num_ ])\n        \n        X_t = {'num': X_num, 'assetCode': X_cat}\n        if self.base_models_cat:\n            meta_features_cat = np.column_stack([\n                np.column_stack([model.predict(X_t) for model in base_models_cat]).mean(axis=1)\n                for base_models_cat in self.base_models_cat_ ])\n            meta_features = np.concatenate((meta_features_num, meta_features_cat), axis=1)\n        else:\n            meta_features = meta_features_num\n            \n        if self.along == True:\n            meta_features = np.concatenate((X_num, meta_features), axis=1)\n            \n        return self.meta_model_.predict(meta_features)\n    \n    def get_meta_features(self,X):\n        X_num = X['num']\n        X_cat = X['assetCode']\n        \n        meta_features_num = np.column_stack([\n            np.column_stack([model.predict(X_num) for model in base_models_num]).mean(axis=1)\n            for base_models_num in self.base_models_num_ ])\n        \n        X_t = {'num': X_num, 'assetCode': X_cat}\n        if self.base_models_cat:\n            meta_features_cat = np.column_stack([\n                np.column_stack([model.predict(X_t) for model in base_models_cat]).mean(axis=1)\n                for base_models_cat in self.base_models_cat_ ])\n            meta_features = np.concatenate((meta_features_num, meta_features_cat), axis=1)\n        else:\n            meta_features = meta_features_num\n            \n        return meta_features\n    \n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b3aaaaa8b20f7a39ab01d7f18e4b0a2327cdfd80"},"cell_type":"code","source":"\n#from sklearn.neighbors import KNeighborsClassifier\n#from sklearn.linear_model import LogisticRegression\n\n#NN = NN_base()\n\n#KN_2 = KNeighborsClassifier(n_neighbors=2)\n#KN_4 = KNeighborsClassifier(n_neighbors=4)\n#KN_8 = KNeighborsClassifier(n_neighbors=8)\n#lr = LogisticRegression()\n\n\n#GBoost = GradientBoostingClassifier(n_estimators=10, learning_rate=0.05,\n#                                   max_depth=6,min_samples_leaf=15, min_samples_split=10,random_state =5, verbose=2)\n#model_lgb_ = lgb.LGBMClassifier(objective='binary',learning_rate=0.05, bagging_fraction = 0.8,\n#                                bagging_freq = 5, n_estimators=10,boosting_type = 'dart',\n#                                num_leaves = 2452, min_child_samples = 212, reg_lambda=0.01)\n\n#model_xgb_ = xgb.XGBClassifier(colsample_bytree=0.4603, gamma=0.0468, \n#                             learning_rate=0.05, max_depth=6, \n#                             min_child_weight=1.7817, n_estimators=10,\n#                             reg_alpha=0.4640, reg_lambda=0.8571,\n#                             subsample=0.5213, silent=1,\n#                             random_state =7, nthread = -1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1c675f8c0b3ef0027cc44cc0ad1254af9ec6e758"},"cell_type":"code","source":"#model_lgb_meta = lgb.LGBMClassifier(objective='binary',learning_rate=0.05, n_estimators=10, bagging_fraction = 0.8,\n#                             bagging_freq = 5, boosting_type = 'dart')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4d0504a4e04212f0da3b6a0ca28dc26177c329b6"},"cell_type":"code","source":"#NN_meta = NN_base(include_cat=False, n_features=13)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a6498720d67194d6ac2e091a15e1608bd17bb4af"},"cell_type":"code","source":"#stacked_averaged_models = StackingAveragedModels(base_models = (('num',model_lgb_),('num',model_xgb_)),\n#                                                 meta_model = ('object',NN_meta), along=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a8d2c29b583c05bb98c0c1ca58a3b8f806fcaa49"},"cell_type":"code","source":"#stacked_averaged_models.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6fc2534ba8ecb92ae87f559658d1a5be7acb05c"},"cell_type":"code","source":"#confidence_valid = stacked_averaged_models.predict(X_valid)[:]*2 -1\n#print(accuracy_score(confidence_valid>0,y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b4153f910e46c406d4ee8117d136e309417fef31"},"cell_type":"code","source":"#from sklearn.metrics import accuracy_score\n#print(accuracy_score(confidence_valid>0,y_valid))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"158097669835b1d4ed18afda45be3da922c54714"},"cell_type":"code","source":"'''\n# calculation of actual metric that is used to calculate final score\nr_valid = r_valid.clip(-1,1) # get rid of outliers. Where do they come from??\nx_t_i = confidence_valid * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean / std\nprint(score_valid)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e605f120778226d47475a290197ebe461049392"},"cell_type":"code","source":"'''\ndays = env.get_prediction_days()\nimport time\n\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    if n_days % 50 == 0:\n        print(n_days,end=' ')\n    \n    t = time.time()\n    assetCode = market_obs_df['assetCode']\n\n    #market_obs_df['price_diff'] = market_obs_df['close'] - market_obs_df['open']\n    #market_obs_df['close_to_open'] =  np.abs(market_obs_df['close'] / market_obs_df['open'])\n    #market_obs_df['assetName_mean_open'] = market_obs_df.groupby('assetName')['open'].transform('mean')\n    #market_obs_df['assetName_mean_close'] = market_obs_df.groupby('assetName')['close'].transform('mean')\n    market_obs_df[num_cols] = market_obs_df[num_cols].fillna(0)\n    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n    #market_obs_df = market_obs_df.loc[:, num_cols].fillna(0).values\n    X = {'num': market_obs_df[num_cols].values}\n    for i,cat in enumerate(cat_cols):\n        market_obs_df[cat+'_encoded'] = market_obs_df[cat].astype(str).apply(lambda x: encode(encoders[i],x))\n        X[cat] = market_obs_df[cat+'_encoded'].values\n    \n    print(X)\n    prep_time += time.time() - t\n    \n    t = time.time()\n    #lp = stacked_averaged_models.predict(X)\n    lp = NN_tmp.predict(X)[:,0]\n    #lp = model_lgb_.predict(X['num'])[:]\n    #lp = model_xgb_.predict(X['num'])[:]\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    confidence = 2 * lp -1\n    preds = pd.DataFrame({'assetCode':assetCode,'confidence':confidence})\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n    \nenv.write_submission_file()\n\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"cd5e9858661b96c465f43a273d4d0972adc9c51e"},"cell_type":"code","source":"'''\ndays = env.get_prediction_days()\nimport time\n\nn_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\npredicted_confidences = np.array([])\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    n_days +=1\n    print(n_days,end=' ')\n    \n    t = time.time()\n\n    market_obs_df['assetCode_encoded'] = market_obs_df[cat].astype(str).apply(lambda x: encode(encoders[0], x))\n\n    market_obs_df[num_cols] = market_obs_df[num_cols].fillna(0)\n    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n    X_num_test = market_obs_df[num_cols]\n    #X_test = {'num':X_num_test}\n    X_cat_test = market_obs_df['assetCode_encoded']\n\n    meta_features_num_test = np.column_stack([\n                np.column_stack([model.predict(X_num_test) for model in base_learners]).mean(axis=1)\n                for base_learners in base_learners_ ])\n    \n    meta_features_num_test_ctb = np.column_stack([\n                np.column_stack([model.predict_proba(X_num_test) for model in base_learners]).mean(axis=1)\n                for base_learners in base_learners_ctb ])\n    \n    X_test = {'num': X_num_test, 'assetCode': X_cat_test}\n    meta_features_num_test_NN = np.column_stack([\n                np.column_stack([model.predict(X_test) for model in base_learners]).mean(axis=1)\n                for base_learners in base_learners_NN ])\n    \n    meta_features_num_test = np.concatenate((meta_features_num_test_NN, meta_features_num_test_ctb, meta_features_num_test), axis=1)    \n    \n    #print(meta_features_num_test.shape)\n    raw_features_test = X_num_test\n    meta_features_test = np.concatenate((raw_features_test, meta_features_num_test), axis=1)\n    meta_features_test = {'num': meta_features_test, 'assetCode': X_cat_test}\n\n    \n    prep_time += time.time() - t\n    \n    t = time.time()\n    #market_prediction = t_cat.predict_proba(X_num_test)[:,1]*2 -1\n    '''\n    market_prediction_ctb = ctb_meta.predict_proba(meta_features_test)[:,1]*2 -1\n    market_prediction = (market_prediction_ctb + market_predcition_NN)/2\n    '''\n    market_prediction = NN_meta.predict(meta_features_test)[:,0]*2 -1\n    #market_prediction = NN_tmp.predict(X_test)[:,0]*2 -1\n    #market_prediction = meta_features_num_test.mean(axis=1)*2 -1\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n\nenv.write_submission_file()\ntotal = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"411cb0115703e0beb9f59c7fd7bd3d090cf93acd"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bf2c1dc8209301e17882af6d5bb81baaa46bc9bc"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b105f0f315b86eff2ea88a3797561433ce96266"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b66f67a474771a278acd8eb1c9fe65a3e5a28be5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}