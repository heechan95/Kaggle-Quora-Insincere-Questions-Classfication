{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2egXe211XRKG"
   },
   "outputs": [],
   "source": [
    "CSV_PATH = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6939,
     "status": "ok",
     "timestamp": 1571941319719,
     "user": {
      "displayName": "이희찬",
      "photoUrl": "",
      "userId": "06255163499262544080"
     },
     "user_tz": -540
    },
    "id": "GTSuC_8jRVg0",
    "outputId": "cb8419ff-a974-4ab2-8094-a8916586af26"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hg3k2uw54aGO"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Activation, GRU\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, GlobalMaxPooling1D, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JXaGnvDu4mr1"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1015944,
     "status": "ok",
     "timestamp": 1571897234212,
     "user": {
      "displayName": "이희찬",
      "photoUrl": "",
      "userId": "06255163499262544080"
     },
     "user_tz": -540
    },
    "id": "JhNVwZ62RBZz",
    "outputId": "c2048d03-b008-4743-dea5-b62d7cfa05c7"
   },
   "outputs": [],
   "source": [
    "#This code snippet is inspired by https://github.com/chakki-works/chakin\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from progressbar import Bar, ETA, FileTransferSpeed, ProgressBar, Percentage, RotatingMarker\n",
    "\n",
    "ROOT_DIR = './'\n",
    "url = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
    "file_name = url.split('/')[-1]\n",
    "save_path = os.path.join(ROOT_DIR, file_name)\n",
    "\n",
    "widgets = ['Test: ', Percentage(), ' ', Bar(marker=RotatingMarker()), ' ', ETA(), ' ', FileTransferSpeed()]\n",
    "pbar = ProgressBar(widgets=widgets)\n",
    "\n",
    "def dlProgress(count, blockSize, totalSize):\n",
    "    if pbar.max_value is None:\n",
    "        pbar.max_value = totalSize\n",
    "        pbar.start()\n",
    "\n",
    "    pbar.update(min(count * blockSize, totalSize))\n",
    "\n",
    "path, _ = urlretrieve(url, save_path, reporthook=dlProgress)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 149391,
     "status": "ok",
     "timestamp": 1571897431889,
     "user": {
      "displayName": "이희찬",
      "photoUrl": "",
      "userId": "06255163499262544080"
     },
     "user_tz": -540
    },
    "id": "aF7yegr6UbQm",
    "outputId": "467c3c29-738b-4dd9-da94-084bd02fe6f1"
   },
   "outputs": [],
   "source": [
    "!unzip ./glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xaalcs2-SbQD"
   },
   "outputs": [],
   "source": [
    "def load_glove(word_dict):#word_index {'word': index} dictionary\n",
    "    EMBEDDING_FILE = './glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    word_index = word_dict\n",
    "    nb_words = len(word_index)+1\n",
    "    \n",
    "\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size),dtype=np.float32) - 1.\n",
    "    \n",
    "    for key, i in tqdm(word_index.items()):\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)        \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "            \n",
    "    del embeddings_index\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    return embedding_matrix, nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Bs1by95JkhF"
   },
   "outputs": [],
   "source": [
    "def load_glove_simple(word_dict):#word_index {'word': index} dictionary\n",
    "    EMBEDDING_FILE = './glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    word_index = word_dict\n",
    "    nb_words = len(word_index)+1\n",
    "    \n",
    "\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size),dtype=np.float32) - 1.\n",
    "    \n",
    "    for key, i in tqdm(word_index.items()):\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word)        \n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            continue\n",
    "            \n",
    "    del embeddings_index\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    return embedding_matrix, nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9130,
     "status": "ok",
     "timestamp": 1571941333204,
     "user": {
      "displayName": "이희찬",
      "photoUrl": "",
      "userId": "06255163499262544080"
     },
     "user_tz": -540
    },
    "id": "EA_V4wC-Vd0S",
    "outputId": "045d28c0-6cf4-4f15-eafc-94beeae2beae"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(os.path.join(CSV_PATH, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(CSV_PATH, 'test.csv'))\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5326,
     "status": "ok",
     "timestamp": 1571941333205,
     "user": {
      "displayName": "이희찬",
      "photoUrl": "",
      "userId": "06255163499262544080"
     },
     "user_tz": -540
    },
    "id": "EKjU3wb1U7t-",
    "outputId": "61a3ab4e-9a18-4d25-c58c-002d9d0911d3"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wq2SJd9fZnXB"
   },
   "outputs": [],
   "source": [
    "train_X = train_df[\"question_text\"].fillna(\" \")\n",
    "test_X = test_df[\"question_text\"].fillna(\" \")\n",
    "text_list = pd.concat([train_X,test_X]).apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3gnT9M8HpeEW"
   },
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.08, stratify=train_df['target'] ,random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrK_NLYFZwtD"
   },
   "outputs": [],
   "source": [
    "max_features = 95000\n",
    "max_len = 100\n",
    "\n",
    "\n",
    "#train_X = np.array(train_df.pop('question_text')) --> This might help to save memory\n",
    "train_X = train_df['question_text'].fillna(\" \")\n",
    "val_X = val_df['question_text'].fillna(\" \")\n",
    "test_X = test_df['question_text'].fillna(\" \")\n",
    "\n",
    "train_y = train_df['target'].values\n",
    "val_y = val_df['target'].values\n",
    "\n",
    "bool_train_labels = train_y != 0\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(text_list))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kls8qClVZy7F"
   },
   "outputs": [],
   "source": [
    "train_X_pad = pad_sequences(train_X, maxlen=100,padding='post')\n",
    "val_X_pad = pad_sequences(val_X, maxlen=100, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 66493,
     "status": "ok",
     "timestamp": 1571941406713,
     "user": {
      "displayName": "이희찬",
      "photoUrl": "",
      "userId": "06255163499262544080"
     },
     "user_tz": -540
    },
    "id": "glV2vKT9bkUB",
    "outputId": "7d1c00a3-4aae-4b7b-8f58-d3ea1ebc91ec"
   },
   "outputs": [],
   "source": [
    "train_X_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 62153,
     "status": "ok",
     "timestamp": 1571941406714,
     "user": {
      "displayName": "이희찬",
      "photoUrl": "",
      "userId": "06255163499262544080"
     },
     "user_tz": -540
    },
    "id": "osVtsDYdaZmy",
    "outputId": "5f0ef773-1153-4388-a3af-07ae2fb61472"
   },
   "outputs": [],
   "source": [
    "#6.19%\n",
    "neg, pos = np.bincount(train_df['target'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O7XmnDSRatHS"
   },
   "outputs": [],
   "source": [
    "dataset_v1 = tf.data.Dataset.from_tensor_slices((train_X_pad, train_y)).shuffle(1000).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0XW_NbNcdiIz"
   },
   "outputs": [],
   "source": [
    "def data_generator():\n",
    "    stop = len(train_X)\n",
    "    i = 0\n",
    "    while i < stop:\n",
    "        yield np.array(train_X[i]), np.array(train_y[i])\n",
    "        i += 1\n",
    "    \n",
    "dataset_v2 = tf.data.Dataset.from_generator(data_generator, output_types=(tf.int32,tf.int32), output_shapes=((None), ()))\n",
    "padded_dataset_v2 = dataset_v2.padded_batch(16, padded_shapes=((None,), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1496,
     "status": "ok",
     "timestamp": 1571941984932,
     "user": {
      "displayName": "이희찬",
      "photoUrl": "",
      "userId": "06255163499262544080"
     },
     "user_tz": -540
    },
    "id": "8mHhzsP5_ad1",
    "outputId": "96786af4-5a17-4f91-f450-fca139aa7d2a"
   },
   "outputs": [],
   "source": [
    "def class_func(features, label):\n",
    "    return label\n",
    "\n",
    "fractions = list(map(lambda x : x.astype(np.float32), [neg/total, pos/total]))\n",
    "print(fractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPL_lWy4_aZh"
   },
   "outputs": [],
   "source": [
    "resampler = tf.data.experimental.rejection_resample(\n",
    "    class_func, target_dist=[0.7, 0.3], initial_dist=fractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1571941990420,
     "user": {
      "displayName": "이희찬",
      "photoUrl": "",
      "userId": "06255163499262544080"
     },
     "user_tz": -540
    },
    "id": "778dpOpnBbec",
    "outputId": "5b164463-a267-4658-89d0-cc9aefeee0ad"
   },
   "outputs": [],
   "source": [
    "resampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNbUDe3J_lZC"
   },
   "outputs": [],
   "source": [
    "resample_ds = dataset_v2.apply(resampler)\n",
    "balanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label).padded_batch(16, padded_shapes=((None,), ()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ChUvVUbdxJtJ"
   },
   "source": [
    "The requirements to use the cuDNN implementation are:\n",
    "\n",
    "\n",
    "activation == 'tanh'\n",
    "\n",
    "recurrent_activation == 'sigmoid'\n",
    "\n",
    "recurrent_dropout == 0\n",
    "\n",
    "unroll is False\n",
    "\n",
    "use_bias is True\n",
    "\n",
    "Inputs are not masked or strictly right padded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 309373,
     "status": "ok",
     "timestamp": 1571916426255,
     "user": {
      "displayName": "이희찬",
      "photoUrl": "",
      "userId": "06255163499262544080"
     },
     "user_tz": -540
    },
    "id": "TvaT7rKg1reG",
    "outputId": "a8dd1836-ad54-4228-ab3e-06e30a01628d"
   },
   "outputs": [],
   "source": [
    "glove, n_words = load_glove_simple(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EV6oaZzcngEG"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.RandomUniform(seed=10000)\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(Attention, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3 #(batch, timestep, features)\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init, trainable=True)\n",
    "\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = tf.matmul(x, self.W)\n",
    "        x_shape = x.shape.as_list()\n",
    "        logits = tf.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = tf.math.exp(logits - tf.math.reduce_max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = tf.dtypes.cast(mask, tf.float32)\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (tf.reduce_sum(ai, axis=1, keepdims=True) + 1e-07)\n",
    "        weighted_input = x * tf.expand_dims(att_weights, axis=-1)\n",
    "        result = tf.math.reduce_sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NJWb6L55mRcY"
   },
   "outputs": [],
   "source": [
    "class QIQModel(tf.keras.Model):\n",
    "    def __init__(self, embedding_matrix, *args, init_out_bias=None, dropout=0.1, **kargs):\n",
    "        super(QIQModel, self).__init__(*args, **kargs)\n",
    "\n",
    "\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.emb_shape = embedding_matrix.shape\n",
    "        self.embedding = Embedding(*self.emb_shape, embeddings_initializer=tf.keras.initializers.Constant(self.embedding_matrix), trainable=False)\n",
    "        self.LSTM = LSTM(64, return_sequences=True)\n",
    "        self.Gmaxpool = GlobalMaxPool1D()\n",
    "        self.linear1 = Dense(16)\n",
    "        if init_out_bias:\n",
    "            self.linear2 = Dense(1, bias_initializer=init_out_bias)\n",
    "        else:\n",
    "            self.linear2 = Dense(1)\n",
    "    \n",
    "    \n",
    "    def call(self, inputs, perturb=False):\n",
    "        emb_out = self.embedding(inputs)\n",
    "        x = self.LSTM(emb_out)\n",
    "        x = self.Gmaxpool(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "    \n",
    "        if perturb:\n",
    "            return activations.sigmoid(x), emb_out\n",
    "        return activations.sigmoid(x)\n",
    "\n",
    "      def compute_output_shape(self, input_shape):\n",
    "        return tf.TensorShape((None, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qyCVdiVGsqsz"
   },
   "outputs": [],
   "source": [
    "initial_bias = np.log([pos/neg])\n",
    "initial_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_fVQcaV1j6T"
   },
   "outputs": [],
   "source": [
    "model = QIQModel(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0yaB7U3RBTV"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/hireme/fun-api-keras-f1-metric-cyclical-learning-rate/code\n",
    "\n",
    "class CyclicLR(tf.keras.callbacks.Callback):\n",
    "    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n",
    "    The method cycles the learning rate between two boundaries with\n",
    "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).\n",
    "    The amplitude of the cycle can be scaled on a per-iteration or \n",
    "    per-cycle basis.\n",
    "    This class has three built-in policies, as put forth in the paper.\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n",
    "        cycle iteration.\n",
    "    For more detail, please see paper.\n",
    "    \n",
    "    # Example\n",
    "        ```python\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., mode='triangular')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```\n",
    "    \n",
    "    Class also supports custom scaling functions:\n",
    "        ```python\n",
    "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))\n",
    "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "                                step_size=2000., scale_fn=clr_fn,\n",
    "                                scale_mode='cycle')\n",
    "            model.fit(X_train, Y_train, callbacks=[clr])\n",
    "        ```    \n",
    "    # Arguments\n",
    "        base_lr: initial learning rate which is the\n",
    "            lower boundary in the cycle.\n",
    "        max_lr: upper boundary in the cycle. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore \n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function.\n",
    "        step_size: number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch.\n",
    "        mode: one of {triangular, triangular2, exp_range}.\n",
    "            Default 'triangular'.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "        gamma: constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "        scale_fn: Custom scaling policy defined by a single\n",
    "            argument lambda function, where \n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored \n",
    "        scale_mode: {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on \n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle). Default is 'cycle'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
    "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
    "        super(CyclicLR, self).__init__()\n",
    "\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "        if scale_fn == None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = lambda x: 1.\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = lambda x: gamma**(x)\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "        self._reset()\n",
    "\n",
    "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
    "               new_step_size=None):\n",
    "        \"\"\"Resets cycle iterations.\n",
    "        Optional boundary/step size adjustment.\n",
    "        \"\"\"\n",
    "        if new_base_lr != None:\n",
    "            self.base_lr = new_base_lr\n",
    "        if new_max_lr != None:\n",
    "            self.max_lr = new_max_lr\n",
    "        if new_step_size != None:\n",
    "            self.step_size = new_step_size\n",
    "        self.clr_iterations = 0.\n",
    "        \n",
    "    def clr(self):\n",
    "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
    "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
    "        if self.scale_mode == 'cycle':\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
    "        else:\n",
    "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs = logs or {}\n",
    "\n",
    "        if self.clr_iterations == 0:\n",
    "            tf.keras.backed.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "        else:\n",
    "            tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())        \n",
    "            \n",
    "    def on_batch_end(self, epoch, logs=None):\n",
    "        \n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "\n",
    "        self.history.setdefault('lr', []).append(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
    "\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        \n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgDmfWaGh9ng"
   },
   "outputs": [],
   "source": [
    "clr = CyclicLR(base_lr=0.001, max_lr=0.002,\n",
    "               step_size=300., mode='exp_range',\n",
    "               gamma=0.99994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nk2Pk2ayh-K4"
   },
   "outputs": [],
   "source": [
    "callbacks = []\n",
    "callbacks += [clr]\n",
    "callbacks += [ tf.keras.callbacks.ModelCheckpoint(filepath='/content/drive/My Drive/Kaggle/Quora Insincere Question/model_{epoch}.h5', mode='min', monitor='val_loss')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZH3Tlb8lpC-"
   },
   "outputs": [],
   "source": [
    "metrics = [\n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PAYpxPSyuEut"
   },
   "outputs": [],
   "source": [
    "weight_for_0 = (1 / neg)*(total)/2.0 \n",
    "weight_for_1 = (1 / pos)*(total)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itC8iq4x1j_3"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01, beta_1=0.99, epsilon=1e-1), loss='binary_crossentropy', metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 115881,
     "status": "error",
     "timestamp": 1571916654537,
     "user": {
      "displayName": "이희찬",
      "photoUrl": "",
      "userId": "06255163499262544080"
     },
     "user_tz": -540
    },
    "id": "MS15utXh36d4",
    "outputId": "17c559f9-6864-4aa2-8da1-35fd235f7260"
   },
   "outputs": [],
   "source": [
    "model.fit(padded_dataset2, verbose=1, epochs=2, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 976,
     "status": "ok",
     "timestamp": 1571912161541,
     "user": {
      "displayName": "이희찬",
      "photoUrl": "",
      "userId": "06255163499262544080"
     },
     "user_tz": -540
    },
    "id": "WxvHQssrOseS",
    "outputId": "4a052ffd-5daa-49ae-f45c-70b864920e43"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mTPByf9saJcI"
   },
   "source": [
    "Try common tecniques for dealing with imbalanced data like:\n",
    "\n",
    "Class weighting\n",
    "\n",
    "Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybpf_undMKPP"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    #if len(targets.shape) > 2:\n",
    "    #  width = targets.shape[-2]\n",
    "    #  targets = tf.reshape(targets, [-1, width*width])\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits_ = pixelrnn(inputs)\n",
    "        #loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=targets, logits=logits_))\n",
    "        bce = tf.keras.losses.BinaryCrossentropy()\n",
    "        loss = bce(targets, logits_)\n",
    "    grads = tape.gradient(loss, pixelrnn.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, pixelrnn.trainable_variables))\n",
    "  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pYbEj2fGMNsu"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 16\n",
    "loss_trace = []\n",
    "import time\n",
    "for epoch in range(EPOCHS):\n",
    "  \n",
    "    total_loss = 0.0\n",
    "    i=0\n",
    "    epoch_start = time.time()\n",
    "    for x,y in train_dataset:\n",
    "        loss = train_step(x,y)\n",
    "    \n",
    "        total_loss += loss\n",
    "        i += 1\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            if i % 500 == 0:\n",
    "                print('{}% complete'.format(i/30))\n",
    "        else:\n",
    "            print('{}% complete'.format(i/30) ,end=' ')\n",
    "        \n",
    "    epoch_elapsed = time.time() - epoch_start\n",
    "    print(\"epoch {} : elapsed: {}\".format(epoch,epoch_elapsed))\n",
    "    print(\"epoch {} : loss: {}\".format(epoch,total_loss))\n",
    "  \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "  \n",
    "  loss_trace.append(total_loss)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Kaggle - Quora insincere Question.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
